{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0252abbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Install Required Libraries\n",
    "!pip install transformers torch accelerate sentencepiece -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1251092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Import Libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d0c8467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "LOADING FLAN-T5 MODEL...\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Using FLAN-T5\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LOADING FLAN-T5 MODEL...\")\n",
    "print(\"=\"*50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ff346d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "417eff19ec8e4b808320c996d93228ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/282 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model 'google/flan-t5-base' loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"google/flan-t5-base\"  # You can also use \"google/flan-t5-small\" for faster loading\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "print(f\"✓ Model '{model_name}' loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4583872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple inference function\n",
    "def answer_question(question, context=\"\"):\n",
    "    \"\"\"\n",
    "    Answer a question using FLAN-T5\n",
    "    \n",
    "    Args:\n",
    "        question: The question to answer\n",
    "        context: Optional context for the question\n",
    "    \"\"\"\n",
    "    if context:\n",
    "        input_text = f\"Answer the question based on the context.\\nContext: {context}\\nQuestion: {question}\"\n",
    "    else:\n",
    "        input_text = f\"Question: {question}\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    # Generate answer\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=150,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    # Decode answer\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f0b4241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING CHATBOT\n",
      "==================================================\n",
      "\n",
      "Q: What is the capital of France?\n",
      "A: monctro\n",
      "\n",
      "Q: What is 25 + 37?\n",
      "A: 0\n",
      "\n",
      "Context: Machine learning is a subset of artificial intelligence that focuses on building systems that can learn from data. It uses algorithms to identify patterns and make decisions with minimal human intervention.\n",
      "Q: What is machine learning?\n",
      "A: building systems that can learn from data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# TESTING THE CHATBOT\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TESTING CHATBOT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test 1: General knowledge question\n",
    "question1 = \"What is the capital of France?\"\n",
    "answer1 = answer_question(question1)\n",
    "print(f\"\\nQ: {question1}\")\n",
    "print(f\"A: {answer1}\")\n",
    "\n",
    "# Test 2: Math question\n",
    "question2 = \"What is 25 + 37?\"\n",
    "answer2 = answer_question(question2)\n",
    "print(f\"\\nQ: {question2}\")\n",
    "print(f\"A: {answer2}\")\n",
    "\n",
    "# Test 3: Context-based question\n",
    "context = \"Machine learning is a subset of artificial intelligence that focuses on building systems that can learn from data. It uses algorithms to identify patterns and make decisions with minimal human intervention.\"\n",
    "question3 = \"What is machine learning?\"\n",
    "answer3 = answer_question(question3, context)\n",
    "print(f\"\\nContext: {context}\")\n",
    "print(f\"Q: {question3}\")\n",
    "print(f\"A: {answer3}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ad683ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "INTERACTIVE Q&A CHATBOT\n",
      "==================================================\n",
      "Type 'quit' or 'exit' to end the conversation\n",
      "Type 'context: <your context>' to set context for questions\n",
      "==================================================\n",
      "\n",
      "Chatbot: antarctica\n",
      "\n",
      "Chatbot: computer system\n",
      "\n",
      "Chatbot: c\n",
      "\n",
      "Chatbot: australia\n",
      "\n",
      "Chatbot: a chemical reaction\n",
      "\n",
      "Chatbot: data warehouse\n",
      "\n",
      "Chatbot: Context set! Now ask me questions about it.\n",
      "\n",
      "Chatbot: allows efficient storage, retrieval\n",
      "\n",
      "Chatbot: efficient storage, retrieval, and management\n",
      "\n",
      "Chatbot: Goodbye! Have a great day!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# INTERACTIVE CHATBOT\n",
    "# ============================================\n",
    "\n",
    "def run_chatbot():\n",
    "    \"\"\"\n",
    "    Interactive chatbot session\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"INTERACTIVE Q&A CHATBOT\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Type 'quit' or 'exit' to end the conversation\")\n",
    "    print(\"Type 'context: <your context>' to set context for questions\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    current_context = \"\"\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"Chatbot: Goodbye! Have a great day!\")\n",
    "            break\n",
    "        \n",
    "        if user_input.lower().startswith('context:'):\n",
    "            current_context = user_input[8:].strip()\n",
    "            print(f\"Chatbot: Context set! Now ask me questions about it.\\n\")\n",
    "            continue\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        # Get answer\n",
    "        answer = answer_question(user_input, current_context)\n",
    "        print(f\"Chatbot: {answer}\\n\")\n",
    "\n",
    "# Run the chatbot\n",
    "run_chatbot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
